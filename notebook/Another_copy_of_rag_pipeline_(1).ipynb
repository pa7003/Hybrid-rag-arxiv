{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vXWxShL5V-qu",
        "outputId": "19992387-2298-47dc-8f70-b31c63c86b9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q faiss-cpu rank-bm25 sentence-transformers pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports & Logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "JeVCX1LeWUuE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "# Logging Setup\n",
        "from src.logging_config import setup_logging\n",
        "setup_logging()\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(\"RAG\")\n",
        "\n",
        "#Load Kaggle Dataset\n",
        "DATA_PATH = \"/content/arxiv_ai.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(DATA_PATH, engine='python', on_bad_lines='skip')\n",
        "    logger.info(f\"Dataset loaded with {len(df)} rows\")\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to load dataset\", exc_info=True)"
      ],
      "metadata": {
        "id": "U2kkM7KeWehg",
        "outputId": "6dd2cf2b-7822-468d-e946-71651c05fdf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3854915778.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Logging Setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msetup_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m logging.basicConfig(\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "def preprocess_dataframe(df):\n",
        "    try:\n",
        "        df = df.fillna(\"\")\n",
        "        df[\"document\"] = (\n",
        "            df[\"title\"].str.lower() + \". \" +\n",
        "            df[\"summary\"].str.lower() + \". \" +\n",
        "            \"categories: \" + df[\"categories\"].str.lower()\n",
        "        )\n",
        "        return df[\"document\"].tolist()\n",
        "    except Exception as e:\n",
        "        logger.error(\"Preprocessing failed\", exc_info=True)\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "ZAtcp1BdY3Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "212d1d7c"
      },
      "source": [
        "documents = preprocess_dataframe(df)\n",
        "logger.info(f\"Prepared {len(documents)} documents\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BM25 Index\n",
        "tokenized_docs = [doc.split() for doc in documents]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n"
      ],
      "metadata": {
        "id": "yO_A8zVdZcp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bm25_search(query, top_k=5):\n",
        "    tokens = query.lower().split()\n",
        "    scores = bm25.get_scores(tokens)\n",
        "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
        "    return [(documents[i], scores[i]) for i in top_idx]\n"
      ],
      "metadata": {
        "id": "TgrwbIpWZl1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings + FAISS\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "Mj9aXlEVZ_zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embeddings = embedder.encode(\n",
        "    documents,\n",
        "    show_progress_bar=True,\n",
        "    batch_size=64\n",
        ")\n"
      ],
      "metadata": {
        "id": "gKiJzNjOaCzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dimension = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(doc_embeddings)\n",
        "\n",
        "logger.info(\"FAISS index built successfully\")\n"
      ],
      "metadata": {
        "id": "WnwK_EtLaFKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query, top_k=5):\n",
        "    query_emb = embedder.encode([query])\n",
        "    distances, indices = index.search(query_emb, top_k)\n",
        "    return [(documents[i], distances[0][pos]) for pos, i in enumerate(indices[0])]\n"
      ],
      "metadata": {
        "id": "PsY2phviaHLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid Search\n",
        "def hybrid_search(query, top_k=5, alpha=0.6):\n",
        "    \"\"\"\n",
        "    alpha -> weight for BM25\n",
        "    (1 - alpha) -> weight for vector similarity\n",
        "    \"\"\"\n",
        "    bm25_results = bm25_search(query, top_k)\n",
        "    vector_results = vector_search(query, top_k)\n",
        "\n",
        "    combined_scores = {}\n",
        "\n",
        "    for doc, score in bm25_results:\n",
        "        combined_scores[doc] = combined_scores.get(doc, 0) + alpha * score\n",
        "\n",
        "    for doc, dist in vector_results:\n",
        "        sim_score = 1 / (1 + dist)   # convert distance → similarity\n",
        "        combined_scores[doc] = combined_scores.get(doc, 0) + (1 - alpha) * sim_score\n",
        "\n",
        "    ranked = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return ranked[:top_k]\n"
      ],
      "metadata": {
        "id": "rrajnWxOaJPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator\n",
        "def generate_answer(query, retrieved_docs):\n",
        "    if not retrieved_docs:\n",
        "        return \"No relevant research papers found.\"\n",
        "\n",
        "    context = \"\\n\\n\".join([doc[:500] for doc, _ in retrieved_docs])\n",
        "\n",
        "    answer = f\"\"\"\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Retrieved Research Context:\n",
        "{context}\n",
        "\n",
        "Answer:\n",
        "Based on the retrieved arXiv AI research papers, the topic mainly discusses the above themes and findings.\n",
        "\"\"\"\n",
        "    return answer.strip()\n"
      ],
      "metadata": {
        "id": "1eIpI1hfaKIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full RAG Pipeline\n",
        "def rag_pipeline(query):\n",
        "    try:\n",
        "        logger.info(f\"Processing query: {query}\")\n",
        "        retrieved_docs = hybrid_search(query)\n",
        "        answer = generate_answer(query, retrieved_docs)\n",
        "        logger.info(\"RAG pipeline completed successfully\")\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        logger.error(\"RAG pipeline failed\", exc_info=True)\n",
        "        return \"An error occurred while processing your query.\"\n"
      ],
      "metadata": {
        "id": "blx8zfFgaOKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Queries\n",
        "test_queries = [\n",
        "    \"Recent advances in transformer models\",\n",
        "    \"Reinforcement learning for robotics\",\n",
        "    \"Explain self supervised learning methods\",\n",
        "    \"Challenges in large language model evaluation\",\n",
        "    \"Ethical issues in artificial intelligence research\"\n",
        "]\n",
        "\n",
        "for q in test_queries:\n",
        "    print(\"=\"*100)\n",
        "    print(rag_pipeline(q))\n"
      ],
      "metadata": {
        "id": "XG0yVRtSaQHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add HuggingFace LLM"
      ],
      "metadata": {
        "id": "IjC6QU8ArnnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "llm = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    max_length=256\n",
        ")\n",
        "\n",
        "def generate_llm_answer(query, retrieved_docs):\n",
        "    context = \" \".join([doc[:300] for doc, _ in retrieved_docs])\n",
        "    prompt = f\"\"\"\n",
        "    Answer the question using the context.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {query}\n",
        "    \"\"\"\n",
        "    return llm(prompt)[0][\"generated_text\"]"
      ],
      "metadata": {
        "id": "ZBeAK--frrth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating RAG pipeline to use LLM\n",
        "def rag_pipeline(query):\n",
        "    retrieved_docs = hybrid_search(query)\n",
        "    answer = generate_llm_answer(query, retrieved_docs)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "OqNkXNSgrzjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_pipeline(\"What are transformer models?\"))\n"
      ],
      "metadata": {
        "id": "xhE0vW-tsb0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Cell\n",
        "def precision_at_k(retrieved, relevant, k):\n",
        "    retrieved_k = retrieved[:k]\n",
        "    return len(set(retrieved_k) & set(relevant)) / k\n",
        "\n",
        "def recall_at_k(retrieved, relevant, k):\n",
        "    retrieved_k = retrieved[:k]\n",
        "    return len(set(retrieved_k) & set(relevant)) / len(relevant)"
      ],
      "metadata": {
        "id": "IXh_NdOoswWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"transformer models\"\n",
        "\n",
        "relevant_docs = [\n",
        "    doc for doc in documents if \"transformer\" in doc\n",
        "][:10]\n",
        "\n",
        "retrieved_docs = [doc for doc, _ in hybrid_search(query, top_k=10)]\n",
        "\n",
        "print(\"Precision@5:\", precision_at_k(retrieved_docs, relevant_docs, 5))\n",
        "print(\"Recall@5:\", recall_at_k(retrieved_docs, relevant_docs, 5))"
      ],
      "metadata": {
        "id": "J-gGY5kOs4pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then in notebook and main.py:\n",
        "\n",
        "from src.logging_config import setup_logging\n",
        "setup_logging()"
      ],
      "metadata": {
        "id": "4xIkjS5RsDVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cR9ZYyv1sGqn"
      }
    }
  ]
}